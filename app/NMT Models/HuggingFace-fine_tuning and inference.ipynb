{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a5123-cff1-47ee-b9c1-43dfbefabc7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "730e258e-0053-4962-82a4-94841b647f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (17.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from pyarrow) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15dc1657-c7f9-4cec-92b8-984070517940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files created successfully: data.en and data.nl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"sentence_level_data-30k.csv\") \n",
    "\n",
    "# Save the English column as data.en\n",
    "df['English'].to_csv(\"data.en\", index=False, header=False) #, quoting=csv.QUOTE_NONE, escapechar='\\\\'\n",
    "\n",
    "# Save the Dutch column as data.nl\n",
    "df['Dutch'].to_csv(\"data.nl\", index=False, header=False) #, quoting=csv.QUOTE_NONE, escapechar='\\\\'\n",
    "\n",
    "print(\"Files created successfully: data.en and data.nl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5829bf26-183e-4e07-bebd-da637e96cee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of characters in a single line in data.en: 2256\n",
      "Maximum number of characters in a single line in data.nl: 1789\n",
      "Maximum length between both files: 2256\n"
     ]
    }
   ],
   "source": [
    "# Load the English and Dutch data\n",
    "with open(\"data.en\", \"r\", encoding=\"utf-8\") as en_file:\n",
    "    en_lines = en_file.readlines()\n",
    "\n",
    "with open(\"data.nl\", \"r\", encoding=\"utf-8\") as nl_file:\n",
    "    nl_lines = nl_file.readlines()\n",
    "\n",
    "# Find the maximum length of a line in either file\n",
    "max_en_length = max(len(line.strip()) for line in en_lines)\n",
    "max_nl_length = max(len(line.strip()) for line in nl_lines)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Maximum number of characters in a single line in data.en: {max_en_length}\")\n",
    "print(f\"Maximum number of characters in a single line in data.nl: {max_nl_length}\")\n",
    "\n",
    "# Get the overall max length from both files\n",
    "max_length = max(max_en_length, max_nl_length)\n",
    "print(f\"Maximum length between both files: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0a377c5-c047-48b4-a654-dc34817916f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': 'This is our precious Auburn.', 'nl': 'Dit is onze dierbare Auburn.'}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Load your data files\n",
    "with open(\"data.en\", \"r\", encoding=\"utf-8\") as f:\n",
    "    english_sentences = [line.strip() for line in f]\n",
    "\n",
    "with open(\"data.nl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dutch_sentences = [line.strip() for line in f]\n",
    "\n",
    "# Verify both files have the same number of lines\n",
    "assert len(english_sentences) == len(dutch_sentences), \"Mismatched number of lines in data.en and data.nl\"\n",
    "\n",
    "# Create a dictionary with the parallel data for translation\n",
    "data = {\"translation\": [{\"en\": src, \"nl\": tgt} for src, tgt in zip(english_sentences, dutch_sentences)]}\n",
    "\n",
    "# Create the Hugging Face dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Check the first example to ensure it's loaded correctly\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d66d0cb7-2ba1-4de9-a900-85f16ee1ccda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 30005/30005 [00:00<00:00, 2085509.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk(\"en-nl-pairs.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "677a32ca-276c-4ae7-a7b5-a669919c054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the dataset from the directory where your arrow files are located\n",
    "dataset = load_from_disk(\"en-nl-pairs.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee5ee9e-b723-4c74-92f5-0d20ca5d6b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': 'This is our precious Auburn.', 'nl': 'Dit is onze dierbare Auburn.'}}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be9accc6-b165-40ff-9ab4-4251f60c9a24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30005/30005 [00:12<00:00, 2329.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-nl\")\n",
    "\n",
    "# Define the tokenization function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"nl\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=1024, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bd3e28a-0647-44eb-aafc-01c894e5ce15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 30005/30005 [00:00<00:00, 84807.27 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# Save the tokenized dataset to a directory\n",
    "tokenized_dataset.save_to_disk(\"tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aec0e7f7-58b8-41f5-a93a-3e7f0fb6f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the dataset from the directory where your arrow files are located\n",
    "tokenized_dataset = load_from_disk(\"tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80d808f7-a929-479f-aa80-d13bf8b3d136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and temporary datasets (80% train, 20% temp)\n",
    "split_data = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Now split the temporary dataset into validation and test sets (50% val, 50% test)\n",
    "temp_dataset = split_data[\"test\"]\n",
    "val_test_split = temp_dataset.train_test_split(test_size=0.5)\n",
    "\n",
    "# Assign the datasets\n",
    "train_dataset = split_data[\"train\"]\n",
    "val_dataset = val_test_split[\"train\"]\n",
    "test_dataset = val_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c74362f0-c990-4307-857a-9e17f69cbef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 24004/24004 [00:00<00:00, 55776.73 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3000/3000 [00:00<00:00, 59549.70 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3001/3001 [00:00<00:00, 41012.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Save the train dataset to a directory\n",
    "train_dataset.save_to_disk(\"train_dataset_hf\")\n",
    "# Save the validation dataset to a directory\n",
    "val_dataset.save_to_disk(\"validation_dataset_hf\")\n",
    "# Save the test dataset to a directory\n",
    "test_dataset.save_to_disk(\"test_dataset_hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "708d8579-b04a-4a6c-bf4c-f93fc8d60da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the dataset from the directory where your arrow files are located\n",
    "train_dataset = load_from_disk(\"train_dataset_hf\")\n",
    "validation_dataset = load_from_disk(\"validation_dataset_hf\")\n",
    "test_dataset = load_from_disk(\"test_dataset_hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d80ba8b-ddaa-46f0-a330-430a538363a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "print(isinstance(train_dataset, datasets.Dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d46f8149-7365-46a2-b240-b3b89bf51be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]C:\\Users\\Simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages\\transformers\\tokenization_utils_base.py:4109: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 3000/3000 [00:01<00:00, 2407.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-nl\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Flatten the translation dictionary\n",
    "    model_inputs = tokenizer(examples['translation']['en'], padding='max_length', truncation=True, max_length=512)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['translation']['nl'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "validating_dataset = validation_dataset.map(preprocess_function, remove_columns=[\"translation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7169ab8b-bad0-431e-b481-d631195caf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 24004/24004 [00:10<00:00, 2395.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-nl\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Flatten the translation dictionary\n",
    "    model_inputs = tokenizer(examples['translation']['en'], padding='max_length', truncation=True, max_length=512)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['translation']['nl'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "training_dataset = train_dataset.map(preprocess_function, remove_columns=[\"translation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35892ce2-1314-4700-8695-575607da05be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3001/3001 [00:01<00:00, 2259.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-nl\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Flatten the translation dictionary\n",
    "    model_inputs = tokenizer(examples['translation']['en'], padding='max_length', truncation=True, max_length=512)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['translation']['nl'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "testing_dataset = test_dataset.map(preprocess_function, remove_columns=[\"translation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fffd632-4cae-41cc-9237-4fe44bd32f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 3000/3000 [00:00<00:00, 171836.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Save the train dataset to a directory\n",
    "validating_dataset.save_to_disk(\"validation_dataset_hf_2transform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccb0bb4b-fd09-4969-9379-7316baa0eb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 24004/24004 [00:00<00:00, 224482.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Save the train dataset to a directory\n",
    "training_dataset.save_to_disk(\"train_dataset_hf_2transform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92fa15f2-27d5-446d-96f3-487345c36b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 3001/3001 [00:00<00:00, 159042.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Save the train dataset to a directory\n",
    "testing_dataset.save_to_disk(\"test_dataset_hf_2transform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8478f3b7-183a-40c4-8a09-edd1276f06c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17de5dcc-75c8-4dd5-8db2-b46b5cb64bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.0+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Number of CUDA devices: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)  # Check PyTorch version\n",
    "print(\"CUDA available:\", torch.cuda.is_available())  # Check if CUDA is available\n",
    "print(\"CUDA version:\", torch.version.cuda)  # CUDA version used by PyTorch\n",
    "print(\"Number of CUDA devices:\", torch.cuda.device_count())  # Number of available CUDA devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4968345-3219-4516-aac2-2ba27c7a47d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a631f1f0-ce32-4d88-b32f-f2dbb6eaf6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.45.2\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7bd1074-ac03-4754-9ba4-573572b5ba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the dataset from the directory where your arrow files are located\n",
    "train_dataset = load_from_disk(\"train_dataset_hf_2transform\")\n",
    "validation_dataset = load_from_disk(\"validation_dataset_hf_2transform\")\n",
    "test_dataset = load_from_disk(\"test_dataset_hf_2transform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "391358d5-bcdb-47ac-89f3-4445ca9e647b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [47460, 161, 2, 643, 2, 337, 2, 0, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [64, 3041, 17181, 331, 41138, 1133, 9012, 21, 10637, 1709, 142, 3, 187, 991, 12, 162, 719, 264, 0, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027, 67027]}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65171172-ea5d-4405-b7c8-a404a2269b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from transformers import MarianMTModel, TrainingArguments, Trainer\n",
    "\n",
    "# Load the MarianMT model\n",
    "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-nl\")\n",
    "# Define training arguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"translation_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    fp16=True,\n",
    "    logging_steps=500,\n",
    "    dataloader_num_workers=3,  # Number of subprocesses to use for data loading.,\n",
    "    save_total_limit=2,  # Keep only the last 2 checkpoints.,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=1000,\n",
    "\n",
    "    logging_dir=\"translation_logs\"\n",
    ")\n",
    "\n",
    "# Set up the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef38bc-9852-4cd7-9fae-399299b98be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.46.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d9b2bd4-0c18-4b4f-9387-1bead04fd3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.1\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f6745-4221-489f-83be-8a8898f4d485",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef81eadf-a1e4-4290-a6d6-86fca5500b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 8.15k/8.15k [00:00<?, ?B/s]\n",
      "100%|██████████| 3001/3001 [55:16<00:00,  1.11s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Result: {'score': 49.401016267244536, 'counts': [39038, 29251, 23318, 18957], 'totals': [58544, 55543, 52556, 49592], 'precisions': [66.68147034708937, 52.66370199665124, 44.367912322094526, 38.225923536054204], 'bp': 1.0, 'sys_len': 58544, 'ref_len': 55808}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "# from datasets import load_metric\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load your fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('fine_tuned_en_nl_translation_model')\n",
    "tokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-nl')\n",
    "\n",
    "# Load your test dataset\n",
    "test_dataset = Dataset.load_from_disk('C:\\\\Users\\\\Simeon\\\\Desktop\\\\UNI\\\\YEAR 3\\\\SEM 1\\\\Software Engineering\\\\Project\\\\Task 2\\\\Fine-tuning\\\\test')\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Setup the metric (e.g., BLEU or ROUGE)\n",
    "metric = evaluate.load(\"sacrebleu\")  # BLEU is common for translation tasks\n",
    "\n",
    "# Function to generate predictions and calculate metric scores\n",
    "def evaluate_model(model, tokenizer, test_dataset, metric):\n",
    "    results = []\n",
    "    references = []\n",
    "    predictions = []\n",
    "    \n",
    "    for example in tqdm(test_dataset):\n",
    "        input_ids = torch.tensor(example['input_ids']).unsqueeze(0)\n",
    "        attention_mask = torch.tensor(example['attention_mask']).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Generate the translation\n",
    "            output_ids = model.generate(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Decode the prediction and references\n",
    "        prediction = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        reference = tokenizer.decode(example['labels'], skip_special_tokens=True)\n",
    "        \n",
    "        # Add to lists for metric calculation\n",
    "        predictions.append(prediction)\n",
    "        references.append([reference])  # SacreBLEU expects a list of references per prediction\n",
    "\n",
    "    # Calculate the metric\n",
    "    metric_result = metric.compute(predictions=predictions, references=references)\n",
    "    return metric_result\n",
    "\n",
    "# Run the evaluation\n",
    "metric_result = evaluate_model(model, tokenizer, test_dataset, metric)\n",
    "\n",
    "# Print the results\n",
    "print(\"Evaluation Result:\", metric_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d97ce5a2-8baf-4ae2-b2de-68178a9f6fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\Simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\Simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.marian.modeling_marian.MarianMTModel'>\n",
      "<class 'transformers.models.marian.tokenization_marian.MarianTokenizer'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Define the directory where the model and tokenizer are stored (adjust if needed)\n",
    "model_dir = 'C:\\\\Users\\\\Simeon\\\\Desktop\\\\UNI\\\\YEAR 3\\\\SEM 1\\\\Software Engineering\\\\Project\\\\Task 2\\\\Fine-tuning\\\\fine_tuned_en_nl_translation_model'\n",
    "\n",
    "# Load your fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)  # Replace with the correct model directory if different\n",
    "tokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-nl')  # Or use your custom tokenizer if saved locally\n",
    "print(type(model))\n",
    "print(type(tokenizer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c02304a1-750c-49ce-9276-c34970076508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 213\n",
      "Input Text: Meet Bella, a gentle, affectionate 4-year-old Labrador mix with a heart as warm as her golden fur. Bella is a loyal companion who’s as happy lounging by your side as she is on a walk in the park. She’s friendly with other dogs and loves meeting new people, making her a wonderful addition to any family. Bella is house-trained, understands basic commands, and has a calm demeanor that’s perfect for quieter households, though she’s always up for some playtime and belly rubs. Recently given a clean bill of health by the vet, she’s all set to find her forever home. Bella’s ideal family would be one that enjoys cuddles and can provide her with daily strolls to satisfy her curious nose. If you’re looking for a devoted friend with endless love to give, Bella might just be the perfect match. Come meet her and see for yourself!\n",
      "\n",
      "Translated Text: Maak kennis met Bella, een zachtaardige, aanhankelijke 4-jarige Labradormix met een hart dat zo warm is als haar gouden vacht. Bella is een loyale metgezel die net zo vrolijk aan je zijde loungt als ze op een wandeling in het park is. Ze is vriendelijk tegen andere honden en houdt van het ontmoeten van nieuwe mensen, waardoor ze een geweldige aanvulling is op elk gezin. Bella is zindelijk, begrijpt basiscommando's en heeft een kalme houding die perfect is voor rustigere huishoudens, hoewel ze altijd opstaat voor wat speeltijd en buikwrijvingen. Onlangs heeft ze een schone gezondheidsverklaring van de dierenarts gekregen, ze is helemaal klaar om haar forever home te vinden. Bella's ideale gezin zou er een zijn die houdt van knuffels en kan haar dagelijks wandelingen geven om haar nieuwsgierige neus te bevredigen. Als je een toegewijde vriendin met eindeloze liefde wilt geven, dan zou Bella gewoon de perfecte match kunnen zijn. Kom haar ontmoeten en zie voor jezelf!\n",
      "\n",
      "Time: 8.275259733200073\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "# Sample text for translation (adjust this as needed)\n",
    "text=\"Meet Bella, a gentle, affectionate 4-year-old Labrador mix with a heart as warm as her golden fur. Bella is a loyal companion who’s as happy lounging by your side as she is on a walk in the park. She’s friendly with other dogs and loves meeting new people, making her a wonderful addition to any family. Bella is house-trained, understands basic commands, and has a calm demeanor that’s perfect for quieter households, though she’s always up for some playtime and belly rubs. Recently given a clean bill of health by the vet, she’s all set to find her forever home. Bella’s ideal family would be one that enjoys cuddles and can provide her with daily strolls to satisfy her curious nose. If you’re looking for a devoted friend with endless love to give, Bella might just be the perfect match. Come meet her and see for yourself!\"\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "input_length = inputs[\"input_ids\"].shape[1]\n",
    "length_penaltyy = 1 + 2.75*(input_length / 100)\n",
    "print(\"Input tokens:\", input_length)\n",
    "print(\"Input Text:\", text)\n",
    "print()\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Generate translation (output will be token ids)\n",
    "with torch.no_grad():\n",
    "    translated_ids = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], num_beams=4, length_penalty=length_penaltyy) #num_beams=3, length_penalty=30\n",
    "\n",
    "# Decode the generated ids to text\n",
    "translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the translated text\n",
    "print(\"Translated Text:\", translated_text)\n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print(\"Time:\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fa58b4-d4af-46c7-827e-0c9e0dbebb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to check a model's max capacity\n",
    "print(model.config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b118782-0d61-4da9-a661-a8c0f49e8cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking token length\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-nl')\n",
    "\n",
    "# Sample sentence\n",
    "sample_text = \"Meet this sweet, loving elderly bulldog who’s recovering from a recent paw surgery. Despite his age, he’s full of gentle affection and enjoys cuddles. He’s currently in rehabilitation, taking slow, steady steps towards recovery. This resilient pup is looking for a cozy home to share his love.\"\n",
    "\n",
    "# Tokenize the text and print token count\n",
    "tokens = tokenizer.encode(sample_text, add_special_tokens=True)\n",
    "print(\"Number of tokens:\", len(tokens))\n",
    "\n",
    "# Decode tokens to check how they match the original text\n",
    "decoded_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "print(\"Decoded text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b3343c39-d4ee-46a1-839b-8ccc6cc99a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Desktop\\UNI\\YEAR 3\\SEM 1\\Software Engineering\\Project\\Task 2\\Fine-tuning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "acf34fd9-8c51-40be-ab7b-c9c9ff2bd5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd30a845-88e2-493f-983a-8e527db6a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --force-reinstall requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0cb4a9d0-39c3-4ccc-8389-22b5918de9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation is consistent across different inferences.\n",
      "Translation successfull, but emoji not handled correctly.\n",
      "Empty input not handled correctly.\n",
      "Model loaded correctly.\n",
      "Tokenizer loaded correctly.\n",
      "Model can translate longer sequences - translation has valid length.\n",
      "Translation successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\Simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\Simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      ".......\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 28.349s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest unit_testing_translation.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
