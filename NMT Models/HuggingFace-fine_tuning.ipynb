{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a5123-cff1-47ee-b9c1-43dfbefabc7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "730e258e-0053-4962-82a4-94841b647f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (17.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from pyarrow) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0a377c5-c047-48b4-a654-dc34817916f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': '\"All citizens of the euro area will have to learn a new unit of account language and how to recognise the new euro coins and banknotes.', 'nl': '\"Alle burgers van de eurozone zullen met de nieuwe munteenheid moeten leren omgaan en de nieuwe euromunststukken en -biljetten kunnen herkennen.'}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Load your data files\n",
    "with open(\"SE4CSAI_2021_Practical/EN-NL/data/data.en\", \"r\", encoding=\"utf-8\") as f:\n",
    "    english_sentences = [line.strip() for line in f]\n",
    "\n",
    "with open(\"SE4CSAI_2021_Practical/EN-NL/data/data.nl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dutch_sentences = [line.strip() for line in f]\n",
    "\n",
    "# Verify both files have the same number of lines\n",
    "assert len(english_sentences) == len(dutch_sentences), \"Mismatched number of lines in data.en and data.nl\"\n",
    "\n",
    "# Create a dictionary with the parallel data for translation\n",
    "data = {\"translation\": [{\"en\": src, \"nl\": tgt} for src, tgt in zip(english_sentences, dutch_sentences)]}\n",
    "\n",
    "# Create the Hugging Face dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Check the first example to ensure it's loaded correctly\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d66d0cb7-2ba1-4de9-a900-85f16ee1ccda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3b8bd156604b88a4d98bb9583c7602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/3528196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"en-nl-pairs.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01265032-8880-4d74-b2a6-44be2a8a8e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y urllib3 boto3 botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98f78d6-0202-4be9-a053-f99c755919a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"urllib3<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27128d0a-bc4d-41c4-bccc-a25faeffa550",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd64ce92-bf1e-46dc-bc84-c458e3e877fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3 version: 1.35.54\n",
      "botocore version: 1.35.54\n",
      "urllib3 version: 1.26.20\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import urllib3\n",
    "\n",
    "print(\"boto3 version:\", boto3.__version__)\n",
    "print(\"botocore version:\", botocore.__version__)\n",
    "print(\"urllib3 version:\", urllib3.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "677a32ca-276c-4ae7-a7b5-a669919c054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the dataset from the directory where your arrow files are located\n",
    "dataset = load_from_disk(\"en-nl-pairs.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ee5ee9e-b723-4c74-92f5-0d20ca5d6b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': '\"All citizens of the euro area will have to learn a new unit of account language and how to recognise the new euro coins and banknotes.', 'nl': '\"Alle burgers van de eurozone zullen met de nieuwe munteenheid moeten leren omgaan en de nieuwe euromunststukken en -biljetten kunnen herkennen.'}}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be9accc6-b165-40ff-9ab4-4251f60c9a24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3528196/3528196 [15:01<00:00, 3912.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-nl\")\n",
    "\n",
    "# Define the tokenization function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"nl\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bd3e28a-0647-44eb-aafc-01c894e5ce15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (14/14 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3528196/3528196 [00:30<00:00, 115587.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Save the tokenized dataset to a directory\n",
    "tokenized_dataset.save_to_disk(\"tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec0e7f7-58b8-41f5-a93a-3e7f0fb6f61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the dataset from the directory where your arrow files are located\n",
    "tokenized_dataset = load_from_disk(\"tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80d808f7-a929-479f-aa80-d13bf8b3d136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and temporary datasets (80% train, 20% temp)\n",
    "split_data = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Now split the temporary dataset into validation and test sets (50% val, 50% test)\n",
    "temp_dataset = split_data[\"test\"]\n",
    "val_test_split = temp_dataset.train_test_split(test_size=0.5)\n",
    "\n",
    "# Assign the datasets\n",
    "train_dataset = split_data[\"train\"]\n",
    "val_dataset = val_test_split[\"train\"]\n",
    "test_dataset = val_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c74362f0-c990-4307-857a-9e17f69cbef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (12/12 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2822556/2822556 [06:24<00:00, 7331.79 examples/s] \n",
      "Saving the dataset (2/2 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 352820/352820 [00:13<00:00, 26586.74 examples/s]\n",
      "Saving the dataset (2/2 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 352820/352820 [00:10<00:00, 34624.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Save the train dataset to a directory\n",
    "train_dataset.save_to_disk(\"train_dataset_hf\")\n",
    "# Save the validation dataset to a directory\n",
    "val_dataset.save_to_disk(\"validation_dataset_hf\")\n",
    "# Save the test dataset to a directory\n",
    "test_dataset.save_to_disk(\"test_dataset_hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "708d8579-b04a-4a6c-bf4c-f93fc8d60da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the dataset from the directory where your arrow files are located\n",
    "train_dataset = load_from_disk(\"train_dataset_hf\")\n",
    "validation_dataset = load_from_disk(\"validation_dataset_hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8478f3b7-183a-40c4-8a09-edd1276f06c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.18.5-py3-none-win_amd64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from wandb) (4.25.5)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from wandb) (2.32.3)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.17.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from wandb) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.18.5-py3-none-win_amd64.whl (15.4 MB)\n",
      "   ---------------------------------------- 0.0/15.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.3/15.4 MB 9.6 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 3.1/15.4 MB 10.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 5.0/15.4 MB 8.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.3/15.4 MB 8.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.6/15.4 MB 8.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 8.9/15.4 MB 7.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.5/15.4 MB 7.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.8/15.4 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.1/15.4 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.2/15.4 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.2/15.4 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.4/15.4 MB 6.7 MB/s eta 0:00:00\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Downloading sentry_sdk-2.17.0-py2.py3-none-any.whl (314 kB)\n",
      "Downloading setproctitle-1.3.3-cp310-cp310-win_amd64.whl (11 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
      "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.17.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.18.5\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e859fe9b-d318-435d-9117-001856a74bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.5.1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Simeon\\AppData\\Roaming\\Python\\Python310\\site-packages\\~orch'."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling torch-2.5.1:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You can safely remove it manually."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully uninstalled torch-2.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94e4202b-5a67-41aa-9175-d893f3bfa05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torch==2.2.0\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torch-2.2.0%2Bcu121-cp310-cp310-win_amd64.whl (2454.8 MB)\n",
      "Requirement already satisfied: filelock in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from torch==2.2.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from torch==2.2.0) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from torch==2.2.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from torch==2.2.0) (3.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from torch==2.2.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from torch==2.2.0) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from jinja2->torch==2.2.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages (from sympy->torch==2.2.0) (1.3.0)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.2+cu121\n",
      "    Uninstalling torch-2.2.2+cu121:\n",
      "      Successfully uninstalled torch-2.2.2+cu121\n",
      "Successfully installed torch-2.2.0+cu121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Simeon\\.conda\\envs\\p3_10_13\\Lib\\site-packages\\~-rch'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.20.1 requires torch==2.5.1, but you have torch 2.2.0+cu121 which is incompatible.\n",
      "autoawq 0.2.6 requires torch==2.3.1, but you have torch 2.2.0+cu121 which is incompatible.\n",
      "autoawq-kernels 0.0.7 requires torch==2.3.1, but you have torch 2.2.0+cu121 which is incompatible.\n",
      "eole 0.0.2 requires torch<2.4,>=2.3, but you have torch 2.2.0+cu121 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.2.0 --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17de5dcc-75c8-4dd5-8db2-b46b5cb64bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.0+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Number of CUDA devices: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)  # Check PyTorch version\n",
    "print(\"CUDA available:\", torch.cuda.is_available())  # Check if CUDA is available\n",
    "print(\"CUDA version:\", torch.version.cuda)  # CUDA version used by PyTorch\n",
    "print(\"Number of CUDA devices:\", torch.cuda.device_count())  # Number of available CUDA devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65171172-ea5d-4405-b7c8-a404a2269b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='164026' max='1411280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 164026/1411280 5:02:11 < 38:17:50, 9.05 it/s, Epoch 0.46/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\.conda\\envs\\p3_10_13\\lib\\site-packages\\transformers\\modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[67027]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from transformers import MarianMTModel, TrainingArguments, Trainer\n",
    "\n",
    "# Load the MarianMT model\n",
    "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-nl\")\n",
    "# Define training arguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"translation_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "    fp16=True,\n",
    "    logging_steps=500,\n",
    "    dataloader_num_workers=4,  # Number of subprocesses to use for data loading.,\n",
    "    save_total_limit=2,  # Keep only the last 2 checkpoints.,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=1000,\n",
    "    logging_dir=\"translation_logs\"\n",
    ")\n",
    "\n",
    "# Set up the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef81eadf-a1e4-4290-a6d6-86fca5500b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
